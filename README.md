# Arabic GPT-J-6B
Fine-tuning the GPT-J-6B model on some Arabic datasets using Jax and its ecosystem.

## Implementation

We will be using the following GPT-J-6B implementation: https://github.com/kingoflolz/mesh-transformer-jax/.
Most likely, due to time constraints we will focus on fine-tuning on some datasets rather than training from scratch.

## Datasets

For now, Arabic Wikipedia.

## First results

Without even fine-tuning, it seems that the GPT-J

## Thanks

Thanks to HuggingFace and the TPU team for organising and providing the infra.

## Links


- https://github.com/kingoflolz/mesh-transformer-jax/
- https://6b.eleuther.ai/
- https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb#scrollTo=nvlAK6RbCJYg
